#Model Building

import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, regularizers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.utils import class_weight
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

# Squash function for capsules
def squash(vectors, axis=-1):
    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis=axis, keepdims=True)
    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + tf.keras.backend.epsilon())
    return scale * vectors

# Capsule Layer with dynamic routing
class CapsuleLayer(layers.Layer):
    def __init__(self, num_capsules, dim_capsule, num_routing=3, **kwargs):
        super(CapsuleLayer, self).__init__(**kwargs)
        self.num_capsules = num_capsules
        self.dim_capsule = dim_capsule
        self.num_routing = num_routing
    
    def build(self, input_shape):
        self.W = self.add_weight(shape=[input_shape[-1], self.num_capsules * self.dim_capsule],
                                 initializer='glorot_uniform', trainable=True)
    
    def call(self, inputs):
        u_hat = tf.einsum('bij,jk->bik', inputs, self.W)
        u_hat = tf.reshape(u_hat, [-1, inputs.shape[1], self.num_capsules, self.dim_capsule])
        u_hat = tf.transpose(u_hat, [0, 2, 1, 3])

        # Routing algorithm
        b = tf.zeros(shape=[tf.shape(u_hat)[0], self.num_capsules, tf.shape(u_hat)[2]])
        for i in range(self.num_routing):
            c = tf.nn.softmax(b, axis=1)
            s = tf.reduce_sum(c[..., tf.newaxis] * u_hat, axis=2)
            v = squash(s)
            if i < self.num_routing - 1:
                b += tf.reduce_sum(u_hat * v[..., tf.newaxis, :], axis=-1)
        
        return v

# Capsule Network Model
def create_capsnet_model(input_shape, num_classes):
    inputs = layers.Input(shape=input_shape)

    # First Conv Layer
    conv1 = layers.Conv2D(64, (9, 9), activation='relu', padding='valid',
                          kernel_regularizer=regularizers.l2(1e-4))(inputs)
    conv1 = layers.BatchNormalization()(conv1)
    conv1 = layers.MaxPooling2D((2, 2))(conv1)
    conv1 = layers.Dropout(0.3)(conv1)

    # Second Conv Layer
    conv2 = layers.Conv2D(64, (9, 9), activation='relu', padding='valid',
                          kernel_regularizer=regularizers.l2(1e-4))(conv1)
    conv2 = layers.BatchNormalization()(conv2)
    conv2 = layers.MaxPooling2D((2, 2))(conv2)
    conv2 = layers.Dropout(0.3)(conv2)

    # Primary Capsule Layer
    primary_capsules = layers.Conv2D(64, (9, 9), strides=2, activation='relu')(conv2)
    primary_capsules = layers.Reshape(target_shape=[-1, 8])(primary_capsules)
    primary_capsules = layers.Lambda(squash)(primary_capsules)

    # Digit Capsule Layer
    digit_capsules = CapsuleLayer(num_capsules=num_classes, dim_capsule=16, num_routing=3)(primary_capsules)
    digit_capsules = layers.Lambda(lambda x: tf.sqrt(tf.reduce_sum(tf.square(x), axis=-1)))(digit_capsules)

    model = models.Model(inputs=inputs, outputs=digit_capsules)
    return model

# Load and preprocess the data
train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    'dataset/train',
    target_size=(260, 260),
    batch_size=32,
    class_mode='categorical',
    color_mode='grayscale'
)

test_generator = test_datagen.flow_from_directory(
    'dataset/test',
    target_size=(260, 260),
    batch_size=32,
    class_mode='categorical',
    color_mode='grayscale'
)

# Model creation
input_shape = (260, 260, 1)  # Grayscale images
num_classes = 2  # Fractured / Non-fractured

model = create_capsnet_model(input_shape, num_classes)

# Compile model with categorical crossentropy
optimizer = optimizers.Adam(learning_rate=1e-5) 
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Class weights to handle imbalance
class_weights = {0: 1.0, 1: 0.9}  

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)

# Training the model
steps_per_epoch = train_generator.samples // train_generator.batch_size
validation_steps = test_generator.samples // test_generator.batch_size

history = model.fit(
    train_generator,
    steps_per_epoch=steps_per_epoch,
    epochs=50,
    validation_data=test_generator,
    validation_steps=validation_steps,
    class_weight=class_weights,
    callbacks=[early_stopping, reduce_lr]
)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(test_generator)
print(f"Test accuracy: {test_accuracy:.4f}")
print(f"Test loss: {test_loss:.4f}")

# Plot accuracy and loss curves
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

plt.show()

# Confusion Matrix and Classification Report
Y_pred = model.predict(test_generator)
y_pred = np.argmax(Y_pred, axis=1)
y_true = test_generator.classes

conf_mat = confusion_matrix(y_true, y_pred)
print("Confusion Matrix:\n", conf_mat)

report = classification_report(y_true, y_pred, target_names=['Non-fractured', 'Fractured'])
print("Classification Report:\n", report)

# Function to visualize predictions
def visualize_predictions(model, test_generator, num_images=5):
    sample_images, sample_labels = next(test_generator)
    predictions = model.predict(sample_images)

    for i in range(num_images):
        plt.imshow(sample_images[i].reshape(260, 260), cmap='gray')
        plt.title(f"Predicted: {np.argmax(predictions[i])}, True: {np.argmax(sample_labels[i])}, Prob: {predictions[i]}")
        plt.axis('off')
        plt.show()

# Visualize predictions for a few test images
#visualize_predictions(model, test_generator)



#Model prediction

import numpy as np
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt

# Load the image, resize to 260x260, and convert to grayscale
def load_and_preprocess_image(img_path):
    img = image.load_img(img_path, target_size=(260, 260), color_mode='grayscale')
    img_array = image.img_to_array(img)  # Convert to numpy array
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension
    img_array = img_array / 255.0  # Normalize to [0, 1]
    return img_array

# Predict the class of the image
def predict_image(img_path, model, class_labels):
    # Preprocess the image
    img_array = load_and_preprocess_image(img_path)
    
    # Make prediction
    prediction = model.predict(img_array)
    
    # The predicted class (fractured or non-fractured)
    predicted_class = np.argmax(prediction, axis=1)[0]
    
    # Print the result
    print(f"Predicted class: {class_labels[predicted_class]}")
    
    # Plot the image
    plt.imshow(image.load_img(img_path, target_size=(260, 260), color_mode='grayscale'), cmap='gray')
    plt.title(f"Prediction: {class_labels[predicted_class]}")
    plt.axis('off')
    plt.show()

# Example usage
img_path = 'example_dataset/download (2).jpeg' 
class_labels = ['non-fractured', 'fractured']  

# Call the prediction function
predict_image(img_path, model, class_labels)

